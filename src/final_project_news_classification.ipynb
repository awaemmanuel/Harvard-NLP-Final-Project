{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Text Classification of MultiNLI Sentences using Multiple Transformer Models  \n",
    "\n",
    "\n",
    "#### TODO\n",
    "1. Make sure that all the packages can be installed on Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/dask/dataframe/utils.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scrapbook as sb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from utils_nlp.common.timer import Timer\n",
    "from utils_nlp.common.pytorch_utils import dataloader_from_dataset\n",
    "from utils_nlp.dataset.multinli import load_pandas_df\n",
    "from utils_nlp.models.transformers.sequence_classification import (\n",
    "    Processor, SequenceClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import urllib\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# your imports may be different\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-5\n",
      "OMP: Info #214: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #156: KMP_AFFINITY: 6 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #285: KMP_AFFINITY: topology layer \"LL cache\" is equivalent to \"socket\".\n",
      "OMP: Info #285: KMP_AFFINITY: topology layer \"L3 cache\" is equivalent to \"socket\".\n",
      "OMP: Info #285: KMP_AFFINITY: topology layer \"L2 cache\" is equivalent to \"core\".\n",
      "OMP: Info #285: KMP_AFFINITY: topology layer \"L1 cache\" is equivalent to \"core\".\n",
      "OMP: Info #285: KMP_AFFINITY: topology layer \"thread\" is equivalent to \"core\".\n",
      "OMP: Info #191: KMP_AFFINITY: 1 socket x 6 cores/socket x 1 thread/core (6 total cores)\n",
      "OMP: Info #216: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to socket 0 core 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to socket 0 core 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to socket 0 core 2 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to socket 0 core 3 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to socket 0 core 4 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to socket 0 core 5 \n",
      "OMP: Info #252: KMP_AFFINITY: pid 60855 tid 60855 thread 0 bound to OS proc set 0\n",
      "Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary folder for data we need during execution of this notebook (we'll clean up\n",
    "# at the end, we promise)\n",
    "# temp_dir = os.path.join(tempfile.gettempdir(), 'mind')\n",
    "\n",
    "data_folder = \"mind_dataset\"\n",
    "temp_dir = data_folder\n",
    "\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# The dataset is split into training and validation set, each with a large and small version.\n",
    "# The format of the four files are the same.\n",
    "# For demonstration purpose, we will use small version validation set only.\n",
    "base_url = 'https://mind201910small.blob.core.windows.net/release'\n",
    "training_small_url = f'{base_url}/MINDsmall_train.zip'\n",
    "validation_small_url = f'{base_url}/MINDsmall_dev.zip'\n",
    "training_large_url = f'{base_url}/MINDlarge_train.zip'\n",
    "validation_large_url = f'{base_url}/MINDlarge_dev.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url,\n",
    "                 destination_filename=None,\n",
    "                 progress_updater=None,\n",
    "                 force_download=False,\n",
    "                 verbose=True):\n",
    "    \"\"\"\n",
    "    Download a URL to a temporary file\n",
    "    \"\"\"\n",
    "    if not verbose:\n",
    "        progress_updater = None\n",
    "    # This is not intended to guarantee uniqueness, we just know it happens to guarantee\n",
    "    # uniqueness for this application.\n",
    "    if destination_filename is None:\n",
    "        url_as_filename = url.replace('://', '_').replace('/', '_')\n",
    "        destination_filename = \\\n",
    "            os.path.join(temp_dir,url_as_filename)\n",
    "    if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "        if verbose:\n",
    "            print('Bypassing download of already-downloaded file {}'.format(\n",
    "                os.path.basename(url)))\n",
    "        return destination_filename\n",
    "    if verbose:\n",
    "        print('Downloading file {} to {}'.format(os.path.basename(url),\n",
    "                                                 destination_filename),\n",
    "              end='')\n",
    "    urllib.request.urlretrieve(url, destination_filename, progress_updater)\n",
    "    assert (os.path.isfile(destination_filename))\n",
    "    nBytes = os.path.getsize(destination_filename)\n",
    "    if verbose:\n",
    "        print('...done, {} bytes.'.format(nBytes))\n",
    "    return destination_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file MINDlarge_train.zip to mind_dataset/https_mind201910small.blob.core.windows.net_release_MINDlarge_train.zip...done, 530196631 bytes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['behaviors.tsv',\n",
       " 'https_mind201910small.blob.core.windows.net_release_MINDlarge_train.zip',\n",
       " 'relation_embedding.vec',\n",
       " '__placeholder__',\n",
       " 'https_mind201910small.blob.core.windows.net_release_MINDsmall_train.zip',\n",
       " 'entity_embedding.vec',\n",
       " '.ipynb_checkpoints',\n",
       " 'news.tsv',\n",
       " 'https_mind201910small.blob.core.windows.net_release_MINDsmall_dev.zip']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For demonstration purpose, we will use small version validation set only.\n",
    "# This file is about 30MB.\n",
    "zip_path = download_url(training_large_url, verbose=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temp_dir)\n",
    "\n",
    "os.listdir(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files with pandas  \n",
    " The news.tsv file contains the detailed information of news articles involved in the behaviors.tsv file.\n",
    " It has 7 columns, which are divided by the tab symbol:\n",
    " - News ID\n",
    " - Category\n",
    " - Subcategory\n",
    " - Title\n",
    " - Abstract\n",
    " - URL\n",
    " - Title Entities (entities contained in the title of this news)\n",
    " - Abstract Entities (entities contained in the abstract of this news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = os.path.join(temp_dir, 'news.tsv')\n",
    "news = pd.read_table(news_path,\n",
    "              header=None,\n",
    "              names=[\n",
    "                  'id', 'category', 'subcategory', 'title', 'abstract', 'url',\n",
    "                  'title_entities', 'abstract_entities'\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "      <td>They seem harmless, but there's a very good re...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAAKEkt.html</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   category      subcategory  \\\n",
       "0  N55528  lifestyle  lifestyleroyals   \n",
       "1  N19639     health       weightloss   \n",
       "2  N61837       news        newsworld   \n",
       "3  N53526     health           voices   \n",
       "4  N38324     health          medical   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "4  How to Get Rid of Skin Tags, According to a De...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "3  I felt like I was a fraud, and being an NBA wi...   \n",
       "4  They seem harmless, but there's a very good re...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "3  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
       "3  [{\"Label\": \"National Basketball Association\", ...  \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51282, 8)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate a number of pretrained models on a subset of the [MultiNLI](https://www.nyu.edu/projects/bowman/multinli/) dataset.\n",
    "\n",
    "We use a [sequence classifier](../../utils_nlp/models/transformers/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/transformers) of different transformers, like [BERT](https://github.com/google-research/bert), [XLNet](https://github.com/zihangdai/xlnet), and [RoBERTa](https://github.com/pytorch/fairseq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "DATA_FOLDER = TemporaryDirectory().name\n",
    "CACHE_DIR = TemporaryDirectory().name\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "MAX_LEN = 100\n",
    "TRAIN_DATA_FRACTION = 0.5\n",
    "TEST_DATA_FRACTION = 0.5\n",
    "TRAIN_SIZE = 0.75\n",
    "LABEL_COL = \"genre\"\n",
    "TEXT_COL = \"sentence1\"\n",
    "MODEL_NAMES = [\"distilbert-base-uncased\", \"roberta-base\", \"xlnet-base-cased\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We start by loading a subset of the data. The following function also downloads and extracts the files, if they don't exist in the data folder.\n",
    "\n",
    "The MultiNLI dataset is mainly used for natural language inference (NLI) tasks, where the inputs are sentence pairs and the labels are entailment indicators. The sentence pairs are also classified into *genres* that allow for more coverage and better evaluation of NLI models.\n",
    "\n",
    "For our classification task, we use the first sentence only as the text input, and the corresponding genre as the label. We select the examples corresponding to one of the entailment labels (*neutral* in this case) to avoid duplicate rows, as the sentences are not unique, whereas the sentence pairs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = news[[\"title\", \"abstract\", \"category\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the data, we observed that there are rows with NaN. This is interesting, because you would expect that from a numerical data and not text. However, we need to spend some time data wrangling and cleaning.  \n",
    "\n",
    "1. First, we remove the rows with Nan\n",
    "1. To avoid scrapping the entire news article with their links, we decided to use a combination of the news title and abstract, from the MIND dataset, as the full text to train our classifiers\n",
    "1. There is a category of article called `news`. We decided to choose the top unique 6 categories removing the `news` category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51277</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51278</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51279</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51280</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51281</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51282 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       title  abstract  category\n",
       "0      False     False     False\n",
       "1      False     False     False\n",
       "2      False     False     False\n",
       "3      False     False     False\n",
       "4      False     False     False\n",
       "...      ...       ...       ...\n",
       "51277  False     False     False\n",
       "51278  False      True     False\n",
       "51279  False     False     False\n",
       "51280  False     False     False\n",
       "51281  False      True     False\n",
       "\n",
       "[51282 rows x 3 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# df = df.replace(np.nan, '', regex=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48616, 3)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df[\"text\"] = df[\"title\"].astype(str) + df[\"abstract\"].astype(str)\n",
    "df.drop(columns=['title', 'abstract'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48616, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[LABEL_COL, TEXT_COL]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict = df.category.value_counts(dropna=True).to_dict()\n",
    "categories_dict = dict(filter(lambda cat: cat[1] > 200, categories_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = (df.category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen = ['news', 'sports', 'finance', 'foodanddrink', 'lifestyle', 'travel']\n",
    "chosen = categories_dict.keys()[:7] - ['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.category.isin(chosen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health</td>\n",
       "      <td>50 Worst Habits For Belly FatThese seemingly h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>health</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>health</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sports</td>\n",
       "      <td>Should NFL be able to fine players for critici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weather</td>\n",
       "      <td>It's been Orlando's hottest October ever so fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news</td>\n",
       "      <td>Chile: Three die in supermarket fire amid prot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Best PS5 games: top PlayStation 5 titles to lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news</td>\n",
       "      <td>How to report weather-related closings, delays...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0      lifestyle  The Brands Queen Elizabeth, Prince Charles, an...\n",
       "1         health  50 Worst Habits For Belly FatThese seemingly h...\n",
       "2           news  The Cost of Trump's Aid Freeze in the Trenches...\n",
       "3         health  I Was An NBA Wife. Here's How It Affected My M...\n",
       "4         health  How to Get Rid of Skin Tags, According to a De...\n",
       "5         sports  Should NFL be able to fine players for critici...\n",
       "6        weather  It's been Orlando's hottest October ever so fa...\n",
       "7           news  Chile: Three die in supermarket fire amid prot...\n",
       "8  entertainment  Best PS5 games: top PlayStation 5 titles to lo...\n",
       "9           news  How to report weather-related closings, delays..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(48597, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(10))\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data for training and testing, sample a fraction for faster execution, and encode the class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/useradmin/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "df_train, df_test = train_test_split(df, train_size = TRAIN_SIZE, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "df_train = df_train.sample(frac=TRAIN_DATA_FRACTION).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=TEST_DATA_FRACTION).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples in the dataset are grouped into 5 genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = 'category'\n",
    "TEXT_COL = 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news             5694\n",
       "sports           4896\n",
       "finance          1126\n",
       "foodanddrink      993\n",
       "lifestyle         871\n",
       "travel            857\n",
       "video             743\n",
       "weather           722\n",
       "health            709\n",
       "autos             599\n",
       "tv                297\n",
       "music             282\n",
       "movies            226\n",
       "entertainment     209\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news             1909\n",
       "sports           1694\n",
       "finance           375\n",
       "foodanddrink      285\n",
       "lifestyle         283\n",
       "travel            280\n",
       "video             268\n",
       "weather           231\n",
       "health            229\n",
       "autos             192\n",
       "tv                108\n",
       "music              84\n",
       "movies             71\n",
       "entertainment      66\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_train[LABEL_COL] = label_encoder.fit_transform(df_train[LABEL_COL])\n",
    "df_test[LABEL_COL] = label_encoder.transform(df_test[LABEL_COL])\n",
    "\n",
    "num_labels = len(np.unique(df_train[LABEL_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 14\n",
      "Number of training examples: 18224\n",
      "Number of testing examples: 6075\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique labels: {}\".format(num_labels))\n",
    "print(\"Number of training examples: {}\".format(df_train.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(df_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display( len(np.unique(df_train[LABEL_COL])))\n",
    "display( len(np.unique(df_test[LABEL_COL])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Pretrained Models\n",
    "\n",
    "Several pretrained models have been made available by [Hugging Face](https://github.com/huggingface/transformers). For text classification, the following pretrained models are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert-base-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert-base-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert-large-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert-large-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert-xlarge-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>xlm-roberta-large-finetuned-conll02-spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>xlm-roberta-large-finetuned-conll03-english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>xlm-roberta-large-finetuned-conll03-german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>xlnet-base-cased</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>xlnet-large-cased</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     model_name\n",
       "0                                albert-base-v1\n",
       "1                                albert-base-v2\n",
       "2                               albert-large-v1\n",
       "3                               albert-large-v2\n",
       "4                              albert-xlarge-v1\n",
       "..                                          ...\n",
       "74  xlm-roberta-large-finetuned-conll02-spanish\n",
       "75  xlm-roberta-large-finetuned-conll03-english\n",
       "76   xlm-roberta-large-finetuned-conll03-german\n",
       "77                             xlnet-base-cased\n",
       "78                            xlnet-large-cased\n",
       "\n",
       "[79 rows x 1 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"model_name\": SequenceClassifier.list_supported_models()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n",
    "\n",
    "Our wrappers make it easy to fine-tune different models in a unified way, hiding the preprocessing details that are needed before training. In this example, we're going to select the following models and use the same piece of code to fine-tune them on our genre classification task. Note that some models were pretrained on multilingual datasets and can be used with non-English datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['distilbert-base-uncased', 'roberta-base', 'xlnet-base-cased']\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAMES[0] in SequenceClassifier.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pretrained model, we preprocess the data, fine-tune the classifier, score the test set, and store the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model =====>  distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   2%|▏         | 10/570 [00:05<04:48,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:29:40, average loss: 2.299556, time duration: 5.246074,\n",
      "                            number of examples in current reporting: 320, step 10\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   4%|▎         | 20/570 [00:10<04:43,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:29:46, average loss: 1.828251, time duration: 5.156210,\n",
      "                            number of examples in current reporting: 320, step 20\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   5%|▌         | 30/570 [00:15<04:41,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:29:51, average loss: 1.480904, time duration: 5.243054,\n",
      "                            number of examples in current reporting: 320, step 30\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   7%|▋         | 40/570 [00:20<04:29,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:29:56, average loss: 1.387157, time duration: 5.108166,\n",
      "                            number of examples in current reporting: 320, step 40\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   9%|▉         | 50/570 [00:25<04:25,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:01, average loss: 1.223475, time duration: 5.128988,\n",
      "                            number of examples in current reporting: 320, step 50\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  11%|█         | 60/570 [00:31<04:24,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:06, average loss: 1.268825, time duration: 5.167764,\n",
      "                            number of examples in current reporting: 320, step 60\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  12%|█▏        | 70/570 [00:36<04:18,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:11, average loss: 1.038846, time duration: 5.184612,\n",
      "                            number of examples in current reporting: 320, step 70\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  14%|█▍        | 80/570 [00:41<04:12,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:17, average loss: 1.060197, time duration: 5.155249,\n",
      "                            number of examples in current reporting: 320, step 80\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  16%|█▌        | 90/570 [00:46<04:08,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:22, average loss: 1.137420, time duration: 5.139284,\n",
      "                            number of examples in current reporting: 320, step 90\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  18%|█▊        | 100/570 [00:51<04:06,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:27, average loss: 1.044322, time duration: 5.224073,\n",
      "                            number of examples in current reporting: 320, step 100\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  19%|█▉        | 110/570 [00:56<03:57,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:32, average loss: 0.959090, time duration: 5.181787,\n",
      "                            number of examples in current reporting: 320, step 110\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  21%|██        | 120/570 [01:02<03:55,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:37, average loss: 0.899837, time duration: 5.226679,\n",
      "                            number of examples in current reporting: 320, step 120\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  23%|██▎       | 130/570 [01:07<03:47,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:42, average loss: 0.826398, time duration: 5.166980,\n",
      "                            number of examples in current reporting: 320, step 130\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  25%|██▍       | 140/570 [01:12<03:44,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:48, average loss: 0.829793, time duration: 5.244097,\n",
      "                            number of examples in current reporting: 320, step 140\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  26%|██▋       | 150/570 [01:17<03:40,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:53, average loss: 1.024284, time duration: 5.247450,\n",
      "                            number of examples in current reporting: 320, step 150\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  28%|██▊       | 160/570 [01:23<03:34,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:30:58, average loss: 0.919913, time duration: 5.208244,\n",
      "                            number of examples in current reporting: 320, step 160\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  30%|██▉       | 170/570 [01:28<03:28,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:03, average loss: 0.893251, time duration: 5.226618,\n",
      "                            number of examples in current reporting: 320, step 170\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  32%|███▏      | 180/570 [01:33<03:24,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:09, average loss: 0.877417, time duration: 5.232664,\n",
      "                            number of examples in current reporting: 320, step 180\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  33%|███▎      | 190/570 [01:38<03:19,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:14, average loss: 0.814778, time duration: 5.248767,\n",
      "                            number of examples in current reporting: 320, step 190\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  35%|███▌      | 200/570 [01:43<03:13,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:19, average loss: 0.795289, time duration: 5.242972,\n",
      "                            number of examples in current reporting: 320, step 200\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  37%|███▋      | 210/570 [01:49<03:09,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:24, average loss: 0.934758, time duration: 5.280176,\n",
      "                            number of examples in current reporting: 320, step 210\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  39%|███▊      | 220/570 [01:54<03:06,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:30, average loss: 0.777315, time duration: 5.319599,\n",
      "                            number of examples in current reporting: 320, step 220\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  40%|████      | 230/570 [01:59<03:01,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:35, average loss: 0.773851, time duration: 5.325432,\n",
      "                            number of examples in current reporting: 320, step 230\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  42%|████▏     | 240/570 [02:05<02:55,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:40, average loss: 1.006330, time duration: 5.324053,\n",
      "                            number of examples in current reporting: 320, step 240\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  44%|████▍     | 250/570 [02:10<02:50,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:46, average loss: 0.783110, time duration: 5.321403,\n",
      "                            number of examples in current reporting: 320, step 250\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  46%|████▌     | 260/570 [02:15<02:44,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:51, average loss: 0.741106, time duration: 5.276181,\n",
      "                            number of examples in current reporting: 320, step 260\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  47%|████▋     | 270/570 [02:21<02:39,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:31:56, average loss: 0.881073, time duration: 5.304994,\n",
      "                            number of examples in current reporting: 320, step 270\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  49%|████▉     | 280/570 [02:26<02:34,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:32:02, average loss: 0.818452, time duration: 5.329850,\n",
      "                            number of examples in current reporting: 320, step 280\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  51%|█████     | 290/570 [02:31<02:29,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:32:07, average loss: 0.788973, time duration: 5.303473,\n",
      "                            number of examples in current reporting: 320, step 290\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  53%|█████▎    | 300/570 [02:37<02:23,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:32:12, average loss: 0.892200, time duration: 5.303161,\n",
      "                            number of examples in current reporting: 320, step 300\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  54%|█████▍    | 310/570 [02:42<02:17,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:32:18, average loss: 0.813662, time duration: 5.287463,\n",
      "                            number of examples in current reporting: 320, step 310\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  56%|█████▌    | 320/570 [02:47<02:12,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:32:23, average loss: 0.819593, time duration: 5.286753,\n",
      "                            number of examples in current reporting: 320, step 320\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  58%|█████▊    | 330/570 [02:52<02:06,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 06/08/2020 01:32:28, average loss: 0.827823, time duration: 5.299879,\n",
      "                            number of examples in current reporting: 320, step 330\n",
      "                            out of total 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  59%|█████▉    | 336/570 [02:56<02:04,  1.88it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-4c1595618148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         classifier.fit(\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_GPUS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sources/Harvard-NLP-Final-Project/src/utils_nlp/models/transformers/sequence_classification.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_dataloader, num_epochs, max_steps, gradient_accumulation_steps, num_gpus, gpu_ids, local_rank, weight_decay, learning_rate, adam_epsilon, warmup_steps, fp16, fp16_opt_level, checkpoint_state_dict, verbose, seed)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mlocal_rank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sources/Harvard-NLP-Final-Project/src/utils_nlp/models/transformers/common.py\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(self, train_dataloader, get_inputs, device, num_gpus, max_steps, global_step, max_grad_norm, gradient_accumulation_steps, optimizer, scheduler, fp16, amp, local_rank, verbose, seed, report_every, save_every, clip_grad_norm, validation_function)\u001b[0m\n\u001b[1;32m    212\u001b[0m                         \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp_gpu/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name in tqdm(MODEL_NAMES[:1], disable=True):\n",
    "\n",
    "    # preprocess\n",
    "    processor = Processor(\n",
    "        model_name=str(name),\n",
    "        to_lower=name.endswith(\"uncased\"),\n",
    "        cache_dir=CACHE_DIR\n",
    "    )\n",
    "    train_dataset = processor.dataset_from_dataframe(\n",
    "        df_train, TEXT_COL, LABEL_COL, max_len=MAX_LEN\n",
    "    )\n",
    "    train_dataloader = dataloader_from_dataset(\n",
    "        train_dataset, batch_size=BATCH_SIZE, num_gpus=NUM_GPUS, shuffle=True\n",
    "    )\n",
    "    test_dataset = processor.dataset_from_dataframe(\n",
    "        df_test, TEXT_COL, LABEL_COL, max_len=MAX_LEN\n",
    "    )\n",
    "    test_dataloader = dataloader_from_dataset(\n",
    "        test_dataset, batch_size=BATCH_SIZE, num_gpus=NUM_GPUS, shuffle=False\n",
    "    )\n",
    "\n",
    "    # fine-tune\n",
    "    classifier = SequenceClassifier(\n",
    "        model_name=name, num_labels=num_labels, cache_dir=CACHE_DIR\n",
    "    )\n",
    "    with Timer() as t:\n",
    "        classifier.fit(\n",
    "            train_dataloader, num_epochs=NUM_EPOCHS, num_gpus=NUM_GPUS, verbose=True,\n",
    "        )\n",
    "    train_time = t.interval / 3600\n",
    "\n",
    "    # predict\n",
    "    preds = classifier.predict(test_dataloader, num_gpus=NUM_GPUS, verbose=True)\n",
    "\n",
    "    # eval\n",
    "    accuracy = accuracy_score(df_test[LABEL_COL], preds)\n",
    "    class_report = classification_report(\n",
    "        df_test[LABEL_COL], preds, target_names=label_encoder.classes_, output_dict=True\n",
    "    )\n",
    "\n",
    "    # save results\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1-score\": class_report[\"macro avg\"][\"f1-score\"],\n",
    "        \"time(hrs)\": train_time,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Finally, we report the accuracy and F1-score metrics for each model, as well as the fine-tuning time in hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distilbert-base-uncased</th>\n",
       "      <th>roberta-base</th>\n",
       "      <th>xlnet-base-cased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.772840</td>\n",
       "      <td>0.783374</td>\n",
       "      <td>0.781399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.632450</td>\n",
       "      <td>0.647413</td>\n",
       "      <td>0.648331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time(hrs)</th>\n",
       "      <td>0.083547</td>\n",
       "      <td>0.164331</td>\n",
       "      <td>0.233440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           distilbert-base-uncased  roberta-base  xlnet-base-cased\n",
       "accuracy                  0.772840      0.783374          0.781399\n",
       "f1-score                  0.632450      0.647413          0.648331\n",
       "time(hrs)                 0.083547      0.164331          0.233440"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.6132075471698113,
       "encoder": "json",
       "name": "accuracy",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "accuracy"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.17142857142857146,
       "encoder": "json",
       "name": "f1",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "f1"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # for testing\n",
    "# sb.glue(\"accuracy\", df_results.iloc[0, :].mean())\n",
    "# sb.glue(\"f1\", df_results.iloc[1, :].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(MODEL_NAMES[2])\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             MODEL_NAMES[0],\n",
    "#             do_lower_case=True,\n",
    "#             cache_dir=CACHE_DIR,\n",
    "#             output_loading_info=False,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = Processor(\n",
    "#     model_name=MODEL_NAMES[0],\n",
    "#     to_lower=True,\n",
    "#     cache_dir=CACHE_DIR,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define model \n",
    "# model_name = MODEL_NAMES[0] # distillbert  uncased\n",
    "# tokenizer_initial_checkpoint = \"bert-base-uncased\"\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_initial_checkpoint)\n",
    "\n",
    "# # Text Classification pipeline, specifying the checkpoint identifier\n",
    "# pipeline('text-classification', model=model_name, tokenizer=tokenizer_initial_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_gpu)",
   "language": "python",
   "name": "nlp_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
